{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"第八章 情感分析.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPdJhLe/tiEhUws4nG6NSDR"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"6R8RFa0GSZ-J","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596415195548,"user_tz":-540,"elapsed":1320,"user":{"displayName":"martina wang","photoUrl":"","userId":"14357932357347601065"}}},"source":["# Sentiment Analysis Using the Movie Ratings Data (Python)\n","\n","# Note that results from this program may differ from the results\n","# documented in the book because algorithms for text parsing\n","# and text classification vary between Python and R. \n","# The objectives of the analysis and steps in completing the analysis\n","# are consistent with those in the book. And results, although\n","# not identical between Python and R, should be very similar.\n","\n","# import packages for text processing and machine learning\n","import os  # operating system commands\n","import re  # regular expressions\n","import nltk  # draw on the Python natural language toolkit\n","import pandas as pd  # DataFrame structure and operations\n","import numpy as np  # arrays and numerical processing\n","import matplotlib.pyplot as plt  # 2D plotting\n","import statsmodels.api as sm  # logistic regression\n","import statsmodels.formula.api as smf  # R-like model specification\n","import patsy  # translate model specification into design matrices\n","from sklearn import svm  # support vector machines\n","from sklearn.ensemble import RandomForestClassifier  # random forests\n","\n","# list files in directory omitting hidden files\n","def listdir_no_hidden(path):\n","    start_list = os.listdir(path)\n","    end_list = []\n","    for file in start_list:\n","        if (not file.startswith('.')):\n","            end_list.append(file)\n","    return(end_list)    \n","\n","# Evaluating Predictive Accuracy of a Binary Classifier (Python)\n","\n","def evaluate_classifier(predicted, observed):\n","    import pandas as pd \n","    if(len(predicted) != len(observed)):\n","        print('\\nevaluate_classifier error:',\\\n","             ' predicted and observed must be the same length\\n')\n","        return(None) \n","    if(len(set(predicted)) != 2):\n","        print('\\nevaluate_classifier error:',\\\n","              ' predicted must be binary\\n')\n","        return(None)          \n","    if(len(set(observed)) != 2):\n","        print('\\nevaluate_classifier error:',\\\n","              ' observed must be binary\\n')\n","        return(None)          \n","\n","    predicted_data = predicted\n","    observed_data = observed\n","    input_data = {'predicted': predicted_data,'observed':observed_data}\n","    input_data_frame = pd.DataFrame(input_data)\n","    \n","    cmat = pd.crosstab(input_data_frame['predicted'],\\\n","        input_data_frame['observed']) \n","    a = float(cmat.ix[0,0])\n","    b = float(cmat.ix[0,1])\n","    c = float(cmat.ix[1,0]) \n","    d = float(cmat.ix[1,1])\n","    n = a + b + c + d\n","    predictive_accuracy = (a + d)/n\n","    true_positive_rate = a / (a + c)\n","    false_positive_rate = b / (b + d)\n","    precision = a / (a + b)\n","    specificity = 1 - false_positive_rate   \n","    expected_accuracy = (((a + b)*(a + c)) + ((b + d)*(c + d)))/(n * n)\n","    kappa = (predictive_accuracy - expected_accuracy)\\\n","       /(1 - expected_accuracy)   \n","    return(a, b, c, d, predictive_accuracy, true_positive_rate, specificity,\\\n","        false_positive_rate, precision, expected_accuracy, kappa)\n","\n","            \n","# Text Measures for Sentiment Analysis (Python)\n","\n","def get_text_measures(corpus):\n","    # individually score each of the twenty-five selected positive words \n","    # for each document in the working corpus... providing new text measures\n","\n","    # initialize the list structures for each positive word\n","    beautiful = []; best =  []; better =  []; classic = [];\n","    enjoy = []; enough = []; entertaining = []; excellent = [];\n","    fans =  []; fun =  []; good =  []; great = []; interesting =  [];  \n","    like =  []; love =  []; nice = []; perfect =  []; pretty =  [];  \n","    right =  []; top = []; well = [];  \n","    won = []; wonderful = [];  work = []; worth = []\n","        \n","    # initialize the list structures for each negative word\n","    bad = []; boring = [];    creepy = [];    dark = []; \n","    dead = []; death =  []; evil = []; fear = []; \n","    funny = []; hard = []; kill = []; killed = []; \n","    lack =  []; lost =  []; mystery = []; plot = []; \n","    poor = []; problem = []; sad = []; scary = []; \n","    slow = []; terrible = []; waste = []; worst = []; wrong  = []\n","\n","    for text in corpus:\n","        beautiful.append(len([w for w in text.split() if w == 'beautiful']))\n","        best.append(len([w for w in text.split() if w == 'best']))\n","        better.append(len([w for w in text.split() if w == 'better']))\n","        classic.append(len([w for w in text.split() if w == 'classic']))\n","\n","        enjoy.append(len([w for w in text.split() if w == 'enjoy']))\n","        enough.append(len([w for w in text.split() if w == 'enough']))\n","        entertaining.append(len([w for w in text.split() if w == 'entertaining']))\n","        excellent.append(len([w for w in text.split() if w == 'excellent']))\n","\n","        fans.append(len([w for w in text.split() if w == 'fans']))\n","        fun.append(len([w for w in text.split() if w == 'fun']))\n","        good.append(len([w for w in text.split() if w == 'good']))\n","        great.append(len([w for w in text.split() if w == 'great']))\n","\n","        interesting.append(len([w for w in text.split() if w == 'interesting']))\n","        like.append(len([w for w in text.split() if w == 'like']))\n","        love.append(len([w for w in text.split() if w == 'love']))\n","        nice.append(len([w for w in text.split() if w == 'nice']))\n","\n","        perfect.append(len([w for w in text.split() if w == 'perfect']))\n","        pretty.append(len([w for w in text.split() if w == 'pretty']))\n","        right.append(len([w for w in text.split() if w == 'right']))\n","        top.append(len([w for w in text.split() if w == 'top']))\n","\n","        well.append(len([w for w in text.split() if w == 'well']))\n","        won.append(len([w for w in text.split() if w == 'won']))\n","        wonderful.append(len([w for w in text.split() if w == 'wonderful']))\n","        work.append(len([w for w in text.split() if w == 'work']))\n","        worth.append(len([w for w in text.split() if w == 'worth']))\n","\n","    # individually score each of the twenty-five selected negative words \n","    # for each document in the working corpus... poviding new text measures\n","   \n","        bad.append(len([w for w in text.split() if w == 'bad']))\n","        boring.append(len([w for w in text.split() if w == 'boring']))\n","        creepy.append(len([w for w in text.split() if w == 'creepy']))\n","        dark.append(len([w for w in text.split() if w == 'dark']))\n","\n","        dead.append(len([w for w in text.split() if w == 'dead']))\n","        death.append(len([w for w in text.split() if w == 'death']))\n","        evil.append(len([w for w in text.split() if w == 'evil']))\n","        fear.append(len([w for w in text.split() if w == 'fear']))\n","\n","        funny.append(len([w for w in text.split() if w == 'funny']))\n","        hard.append(len([w for w in text.split() if w == 'hard']))\n","        kill.append(len([w for w in text.split() if w == 'kill']))\n","        killed.append(len([w for w in text.split() if w == 'killed']))\n","\n","        lack.append(len([w for w in text.split() if w == 'lack']))\n","        lost.append(len([w for w in text.split() if w == 'lost']))\n","        mystery.append(len([w for w in text.split() if w == 'mystery']))\n","        plot.append(len([w for w in text.split() if w == 'plot']))\n","\n","        poor.append(len([w for w in text.split() if w == 'poor']))\n","        problem.append(len([w for w in text.split() if w == 'problem']))\n","        sad.append(len([w for w in text.split() if w == 'sad']))\n","        scary.append(len([w for w in text.split() if w == 'scary']))\n","\n","        slow.append(len([w for w in text.split() if w == 'slow']))\n","        terrible.append(len([w for w in text.split() if w == 'terrible']))\n","        waste.append(len([w for w in text.split() if w == 'waste']))\n","        worst.append(len([w for w in text.split() if w == 'worst']))\n","        wrong.append(len([w for w in text.split() if w == 'wrong']))\n","\n","    # creat dictionary data structure as a preliminary \n","    # to creating the data frame for the fifty text measures\n","    add_corpus_data = {'beautiful':beautiful,'best':best,'better':better,\\\n","        'classic':classic, 'enjoy':enjoy, 'enough':enough,\\\n","        'entertaining':entertaining, 'excellent':excellent,\\\n","        'fans':fans, 'fun':fun, 'good':good, 'great':great,\\\n","        'interesting':interesting, 'like':like, 'love':love, 'nice':nice,\\\n","        'perfect':perfect, 'pretty':pretty, 'right':right, 'top':top,\\\n","        'well':well, 'won':won, 'wonderful':wonderful, 'work':work,\\\n","        'worth':worth,'bad':bad, 'boring':boring, 'creepy':creepy,\\\n","        'dark':dark, 'dead':dead, 'death':death, 'evil':evil, 'fear':fear,\\\n","        'funny':funny,'hard':hard, 'kill':kill, 'killed':killed, 'lack':lack,\\\n","        'lost':lost, 'mystery':mystery, 'plot':plot,'poor':poor,\\\n","        'problem':problem, 'sad':sad, 'scary':scary, 'slow':slow,\\\n","        'terrible':terrible, 'waste':waste, 'worst':worst, 'wrong':wrong}    \n","     \n","    return(add_corpus_data)     \n","    \n","# Summative Scoring of Sentiment (Python)\n","\n","def get_summative_scores(corpus):\n","    # individually score each of the positive and negative words/items \n","    # for each document in the working corpus... \n","    # providing a summative score \n","     \n","    summative_score = []  # intialize list for summative scores\n","    \n","    for text in corpus:\n","        score = 0  # initialize for individual document\n","        # for each document in the working corpus... \n","        # individually score each of the eight selected positive words \n","        if (len([w for w in text.split() if w == 'beautiful']) > 0):\n","            score = score +1\n","        if (len([w for w in text.split() if w == 'best']) > 0):\n","            score = score +1\n","        if (len([w for w in text.split() if w == 'classic']) > 0):\n","            score = score +1\n","        if (len([w for w in text.split() if w == 'excellent']) > 0):\n","            score = score +1\n","        if (len([w for w in text.split() if w == 'great']) > 0):\n","            score = score +1\n","        if (len([w for w in text.split() if w == 'perfect']) > 0):\n","            score = score +1\n","        if (len([w for w in text.split() if w == 'well']) > 0):\n","            score = score +1\n","        if (len([w for w in text.split() if w == 'wonderful']) > 0):\n","            score = score +1\n","            \n","    # individually score each of the ten selected negative words \n","   \n","        if (len([w for w in text.split() if w == 'bad']) > 0):\n","            score = score -1\n","        if (len([w for w in text.split() if w == 'boring']) > 0):\n","            score = score -1\n","        if (len([w for w in text.split() if w == 'funny']) > 0):\n","            score = score -1\n","        if (len([w for w in text.split() if w == 'lack']) > 0):\n","            score = score -1\n","        if (len([w for w in text.split() if w == 'plot']) > 0):\n","            score = score -1\n","        if (len([w for w in text.split() if w == 'poor']) > 0):\n","            score = score -1\n","        if (len([w for w in text.split() if w == 'problem']) > 0):\n","            score = score -1\n","        if (len([w for w in text.split() if w == 'terrible']) > 0):\n","            score = score -1\n","        if (len([w for w in text.split() if w == 'waste']) > 0):\n","            score = score -1\n","        if (len([w for w in text.split() if w == 'worst']) > 0):\n","            score = score -1\n","        \n","        summative_score.append(score)\n","        \n","    summative_score_data = {'summative_score': summative_score}\n","    return(summative_score_data)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"b0BTTyENSZ6d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"status":"ok","timestamp":1596415197987,"user_tz":-540,"elapsed":641,"user":{"displayName":"martina wang","photoUrl":"","userId":"14357932357347601065"}},"outputId":"4d57799e-1790-4237-bb4e-a08304e5fdbf"},"source":["# define list of codes to be dropped from document\n","# carriage-returns, line-feeds, tabs\n","codelist = ['\\r', '\\n', '\\t']    \n","\n","# there are certain words we will ignore in subsequent\n","# text processing... these are called stop-words \n","# and they consist of prepositions, pronouns, and \n","# conjunctions, interrogatives, ...\n","# we begin with the list from the natural language toolkit\n","# examine this initial list of stopwords\n","nltk.download('stopwords')\n","\n","# let's look at that list \n","print(nltk.corpus.stopwords.words('english'))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OU4LzJygbWJ7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596415199095,"user_tz":-540,"elapsed":451,"user":{"displayName":"martina wang","photoUrl":"","userId":"14357932357347601065"}}},"source":["# previous analysis of a list of top terms showed a number of words, along \n","# with contractions and other word strings to drop from further analysis, we add\n","# these to the usual English stopwords to be dropped from a document collection\n","more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n","    'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n","    've', 're', 'vs'] \n","\n","some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n","    'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n","    'toni','welles','william','wolheim','nikita']\n","\n","# start with the initial list and add to it for movie text work \n","stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n","    some_proper_nouns_to_remove\n","\n","# text parsing function for creating text documents \n","# there is more we could do for data preparation \n","# stemming... looking for contractions... possessives... \n","# but we will work with what we have in this parsing function\n","# if we want to do stemming at a later time, we can use\n","#     porter = nltk.PorterStemmer()  \n","# in a construction like this\n","#     words_stemmed =  [porter.stem(word) for word in initial_words]  \n","def text_parse(string):\n","    # replace non-alphanumeric with space \n","    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n","    # replace codes with space\n","    for i in range(len(codelist)):\n","        stopstring = ' ' + codelist[i] + '  '\n","        temp_string = re.sub(stopstring, '  ', temp_string)      \n","    # replace single-character words with space\n","    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n","    # convert uppercase to lowercase\n","    temp_string = temp_string.lower()    \n","    # replace selected character strings/stop-words with space\n","    for i in range(len(stoplist)):\n","        stopstring = ' ' + str(stoplist[i]) + ' '\n","        temp_string = re.sub(stopstring, ' ', temp_string)        \n","    # replace multiple blank characters with one blank character\n","    temp_string = re.sub('\\s+', ' ', temp_string)    \n","    return(temp_string)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"oi6lnVwJbljL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596415201048,"user_tz":-540,"elapsed":665,"user":{"displayName":"martina wang","photoUrl":"","userId":"14357932357347601065"}}},"source":["# read in positive and negative word lists from Hu and Liu (2004)\n","with open('Hu_Liu_positive_word_list.txt','rt') as f:\n","    positive_word_list = f.read().split() "],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6eLgkLHaXEu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596415202291,"user_tz":-540,"elapsed":670,"user":{"displayName":"martina wang","photoUrl":"","userId":"14357932357347601065"}}},"source":["with open('Hu_Liu_negative_word_list.txt','rt',encoding = \"ISO-8859-1\") as f:\n","    negative_word_list = f.read().split()   "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"ugdbCRDQSZ24","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":413},"executionInfo":{"status":"error","timestamp":1596415203696,"user_tz":-540,"elapsed":557,"user":{"displayName":"martina wang","photoUrl":"","userId":"14357932357347601065"}},"outputId":"11c898e3-04ee-474e-effa-4330694e3e55"},"source":["# define counts of positive, negative, and total words in text document \n","def count_positive(text):    \n","    positive = [w for w in text.split() if w in positive_word_list]\n","    return(len(positive))\n","\n","# define text measure for negative score as percentage of negative words   \n","def count_negative(text):    \n","    negative = [w for w in text.split() if w in negative_word_list]\n","    return(len(negative))\n","    \n","# count number of words   \n","def count_total(text):    \n","    total = [w for w in text.split()]\n","    return(len(total))    \n","\n","# define text measure for positive score as percentage of positive words    \n","def score_positive(text):    \n","    positive = [w for w in text.split() if w in positive_word_list]\n","    total = [w for w in text.split()]\n","    return 100 * len(positive)/len(total)\n","\n","# define text measure for negative score as percentage of negative words   \n","def score_negative(text):    \n","    negative = [w for w in text.split() if w in negative_word_list]\n","    total = [w for w in text.split()]\n","    return 100 * len(negative)/len(total)\n","\n","def compute_scores(corpus):\n","    # use the complete word lists for POSITIVE and NEGATIVE measures\n","    # to score all documents in a corpus or list of documents\n","    positive = []\n","    negative = []\n","    for document in corpus:\n","        positive.append(score_positive(document)) \n","        negative.append(score_negative(document)) \n","    return(positive, negative)\n","                           \n","# we use movie ratings data from Mass et al. (2011) \n","# available at http://ai.stanford.edu/~amaas/data/sentiment/\n","# we set up a directory under our working directory structure\n","# /reviews/train/unsup/ for the unsupervised reviews\n","# /reviews/train/neg/ training set negative reviews\n","# /reviews/train/pos/ training set positive reviews\n","# /reviews/test/neg/ text set negative reviews\n","# /reviews/test/pos/ test set positive reviews\n","# /reviews/test/tom/ eight movie reviews from Tom\n","\n","# function for creating corpus and aggregate document \n","# input is directory path for documents\n","# document parsing accomplished by text_parse function\n","# directory of parsed files set up for manual inspection\n","def corpus_creator (input_directory_path, output_directory_path):\n","    # identify the file names in unsup directory\n","    file_names = listdir_no_hidden(path = input_directory_path)\n","    # create list structure for storing parsed documents \n","    document_collection = [] \n","    # initialize aggregate document for all documents in set\n","    aggregate_document = ''\n","    # create a directory for parsed files \n","    parsed_file_directory = output_directory_path\n","    os.mkdir(parsed_file_directory)\n","    # parse each file and write to directory of parsed files\n","    for filename in file_names:\n","        with open(os.path.join(input_directory_path, filename), 'r') as infile:        \n","            this_document = text_parse(infile.read())\n","            aggregate_document = aggregate_document + this_document\n","            document_collection.append(this_document)\n","            outfile = parsed_file_directory + filename\n","            with open(outfile, 'wt') as f:\n","                f.write(str(this_document)) \n","    aggregate_words = [w for w in aggregate_document.split()]   \n","    aggregate_corpus = nltk.Text(aggregate_words)    \n","    return(file_names, document_collection, aggregate_corpus)\n","    \n","# function for extracting rating from file name\n","# for file names of the form 'x_y.txt' where y is the rating\n","def get_rating(string):\n","    return(int(string.partition('.')[0].partition('_')[2])) \n","    \n","# dictionary for mapping of ratings to thumbsupdown \n","map_to_thumbsupdown = {1:'DOWN', 2:'DOWN', 3:'DOWN', 4:'DOWN',\n","    6:'UP', 7:'UP', 8:'UP', 9:'UP', 10:'UP'}     \n","    \n","# begin working with the unsup corpus\n","unsup_file_names, unsup_corpus, unsup_aggregate_corpus = \\\n","    corpus_creator(input_directory_path = './reviews/train/unsup/',\\\n","        output_directory_path = './reviews/train/unsup_parsed/')\n","                    \n","# examine frequency distribution of words in unsup corpus\n","unsup_freq = nltk.FreqDist(unsup_aggregate_corpus)\n","print('\\nNumber of Unique Words in unsup corpus',len(unsup_freq.keys()))\n","print('\\nTop Fifty Words in unsup Corpus:',unsup_freq.keys()[0:50])"],"execution_count":9,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-d76721ce8041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# begin working with the unsup corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0munsup_file_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsup_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsup_aggregate_corpus\u001b[0m \u001b[0;34m=\u001b[0m     \u001b[0mcorpus_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_directory_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./reviews/train/unsup/'\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0moutput_directory_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./reviews/train/unsup_parsed/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# examine frequency distribution of words in unsup corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-d76721ce8041>\u001b[0m in \u001b[0;36mcorpus_creator\u001b[0;34m(input_directory_path, output_directory_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorpus_creator\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_directory_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_directory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# identify the file names in unsup directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistdir_no_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_directory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;31m# create list structure for storing parsed documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mdocument_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-19000f81a78c>\u001b[0m in \u001b[0;36mlistdir_no_hidden\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# list files in directory omitting hidden files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlistdir_no_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mstart_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mend_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstart_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './reviews/train/unsup/'"]}]},{"cell_type":"code","metadata":{"id":"Hqm8y-E5SZyB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1596415179852,"user_tz":-540,"elapsed":705,"user":{"displayName":"martina wang","photoUrl":"","userId":"14357932357347601065"}},"outputId":"0c307278-2358-438a-cfed-502f2e349f9b"},"source":["# identify the most frequent unsup words from the positive word list\n","# here we use set intersection to find a list of the top 25 positive words \n","length_test = 0  # initialize test length\n","nkeys = 0  # slicing index for frequency table extent\n","while (length_test < 25):\n","    length_test =\\\n","        len(set(unsup_freq.keys()[:nkeys]) & set(positive_word_list))\n","    nkeys = nkeys + 1\n","selected_positive_set =\\\n","    set(unsup_freq.keys()[:nkeys]) & set(positive_word_list)\n","selected_positive_words = list(selected_positive_set)\n","selected_positive_words.sort()\n","print('\\nSelected Positive Words:', selected_positive_words)"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b74974f29fe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# slicing index for frequency table extent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlength_test\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlength_test\u001b[0m \u001b[0;34m=\u001b[0m        \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsup_freq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_word_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnkeys\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mselected_positive_set\u001b[0m \u001b[0;34m=\u001b[0m    \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsup_freq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_word_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'unsup_freq' is not defined"]}]},{"cell_type":"code","metadata":{"id":"FdqI4jGOSZtk","colab_type":"code","colab":{}},"source":["# identify the most frequent unsup words from the negative word list\n","# here we use set intersection to find a list of the top 25 negative words \n","length_test = 0  # initialize test length\n","nkeys = 0  # slicing index for frequency table extent\n","while (length_test < 25):\n","    length_test =\\\n","        len(set(unsup_freq.keys()[:nkeys]) & set(negative_word_list))\n","    nkeys = nkeys + 1\n","selected_negative_set =\\\n","    set(unsup_freq.keys()[:nkeys]) & set(negative_word_list)\n","# list is actually 26 items and contains both 'problem' and 'problems'\n","# so we will eliminate 'problems' from the selected negative words\n","selected_negative_set.remove('problems')\n","selected_negative_words = list(selected_negative_set)\n","selected_negative_words.sort()\n","print('\\nSelected Negative Words:', selected_negative_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdKrG9AISZpw","colab_type":"code","colab":{}},"source":["# use the complete word lists for POSITIVE and NEGATIVE measures/scores\n","positive, negative = compute_scores(unsup_corpus)\n","\n","# create data frame to explore POSITIVE and NEGATIVE measures\n","unsup_data = {'file': unsup_file_names,\\\n","    'POSITIVE': positive, 'NEGATIVE': negative}    \n","unsup_data_frame = pd.DataFrame(unsup_data)\n","\n","# summary of distributions of POSITIVE and NEGATIVE scores for unsup corpus\n","print(unsup_data_frame.describe())\n","\n","print('\\nCorrelation between POSITIVE and NEGATIVE',\\\n","    round(unsup_data_frame['POSITIVE'].corr(unsup_data_frame['NEGATIVE']),3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8oyZyOFmSZl7","colab_type":"code","colab":{}},"source":["# scatter plot of POSITIVE and NEGATIVE scores for unsup corpus\n","ax = plt.axes()\n","ax.scatter(unsup_data_frame['NEGATIVE'], unsup_data_frame['POSITIVE'],\\\n","    facecolors = 'none', edgecolors = 'blue')\n","ax.set_xlabel('NEGATIVE')\n","ax.set_ylabel('POSITIVE')   \n","plt.savefig('fig_sentiment_text_measures_scatter_plot.pdf', \n","    bbox_inches = 'tight', dpi=None, facecolor='none', edgecolor='blue', \n","    orientation='portrait', papertype=None, format=None, \n","    transparent=True, pad_inches=0.25, frameon=None)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHCS6tdzSZiK","colab_type":"code","colab":{}},"source":["# work on the directory of training files-----------------------------------\n","# Perhaps POSITIVE and NEGATIVE can be combined in a way to yield effective \n","# predictions of movie ratings. Let us move to a set of movie reviews for \n","# supervised learning.  We select the 500 records from a set of positive \n","# reviews (ratings between 7 and 10) and 500 records from a set of negative \n","# reviews (ratings between 1 and 4). We begin with the training data.\n","\n","# /reviews/train/pos/ training set positive reviews \n","train_pos_file_names, train_pos_corpus, train_pos_aggregate_corpus = \\\n","    corpus_creator(input_directory_path = 'reviews/train/pos/',\\\n","        output_directory_path = 'reviews/train/pos_parsed/')\n","# use the complete word lists for POSITIVE and NEGATIVE measures/scores\n","positive, negative = compute_scores(train_pos_corpus)\n","rating = []\n","for file_name in train_pos_file_names:\n","    rating.append(get_rating(str(file_name)))\n","\n","# create data frame to explore POSITIVE and NEGATIVE measures        \n","train_pos_data = {'train_test':['TRAIN'] * len(train_pos_file_names),\\\n","    'pos_neg': ['POS'] * len(train_pos_file_names),\\\n","    'file_name': train_pos_file_names,\\\n","    'POSITIVE': positive, 'NEGATIVE': negative,\\\n","    'rating': rating}        \n","train_pos_data_frame = pd.DataFrame(train_pos_data)\n","\n","# /reviews/train/neg/ training set negative reviews\n","train_neg_file_names, train_neg_corpus, train_neg_aggregate_corpus = \\\n","    corpus_creator(input_directory_path = 'reviews/train/neg/',\\\n","        output_directory_path = 'reviews/train/neg_parsed/')\n","# use the complete word lists for POSITIVE and NEGATIVE measures/scores\n","positive, negative = compute_scores(train_neg_corpus)\n","rating = []\n","for file_name in train_neg_file_names:\n","    rating.append(get_rating(str(file_name)))\n","\n","# create data frame to explore POSITIVE and NEGATIVE measures        \n","train_neg_data = {'train_test':['TRAIN'] * len(train_neg_file_names),\\\n","    'pos_neg': ['NEG'] * len(train_neg_file_names),\\\n","    'file_name': train_neg_file_names,\\\n","    'POSITIVE': positive, 'NEGATIVE': negative,\\\n","    'rating': rating}        \n","train_neg_data_frame = pd.DataFrame(train_neg_data)\n","\n","# merge the positive and negative training data frames\n","train_data_frame = pd.concat([train_pos_data_frame, train_neg_data_frame],\\\n","    axis = 0, ignore_index = True)\n","\n","# determining thumbs up or down based on rating\n","train_data_frame['thumbsupdown'] = \\\n","    train_data_frame['rating'].map(map_to_thumbsupdown)\n","# compute simple measure of sentiment as POSITIVE - NEGATIVE\n","train_data_frame['simple'] = \\\n","    train_data_frame['POSITIVE'] - train_data_frame['NEGATIVE']    \n","# examine the data frame\n","print(pd.crosstab(train_data_frame['pos_neg'],\\\n","    train_data_frame['thumbsupdown']))\n","print(train_data_frame.head())\n","print(train_data_frame.tail())\n","print(train_data_frame.describe())\n","ratings_grouped = train_data_frame['simple'].\\\n","    groupby(train_data_frame['rating'])\n","print('\\nTraining Data Simple Difference Means by Ratings:',\\\n","    ratings_grouped.mean())\n","thumbs_grouped = \\\n","    train_data_frame['simple'].groupby(train_data_frame['thumbsupdown'])\n","print('\\nTraining Data Simple Difference Means by Thumbs UP/DOWN:',\\\n","    thumbs_grouped.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D9-dLmk5SZdS","colab_type":"code","colab":{}},"source":["# repeat methods for the test data -----------------------------\n","# /reviews/test/pos/ testing set positive reviews\n","test_pos_file_names, test_pos_corpus, test_pos_aggregate_corpus = \\\n","    corpus_creator(input_directory_path = 'reviews/test/pos/',\\\n","        output_directory_path = 'reviews/test/pos_parsed/')\n","# use the complete word lists for POSITIVE and NEGATIVE measures/scores\n","positive, negative = compute_scores(test_pos_corpus)\n","rating = []\n","for file_name in test_pos_file_names:\n","    rating.append(get_rating(str(file_name)))\n","\n","# create data frame to explore POSITIVE and NEGATIVE measures        \n","test_pos_data = {'train_test':['TEST'] * len(test_pos_file_names),\\\n","    'pos_neg': ['POS'] * len(test_pos_file_names),\\\n","    'file_name': test_pos_file_names,\\\n","    'POSITIVE': positive, 'NEGATIVE': negative,\\\n","    'rating': rating}        \n","test_pos_data_frame = pd.DataFrame(test_pos_data)\n","\n","# /reviews/test/neg/ testing set negative reviews\n","test_neg_file_names, test_neg_corpus, test_neg_aggregate_corpus = \\\n","    corpus_creator(input_directory_path = 'reviews/test/neg/',\\\n","        output_directory_path = 'reviews/test/neg_parsed/')\n","# use the complete word lists for POSITIVE and NEGATIVE measures/scores\n","positive, negative = compute_scores(test_neg_corpus)\n","rating = []\n","for file_name in test_neg_file_names:\n","    rating.append(get_rating(str(file_name)))\n","\n","# create data frame to explore POSITIVE and NEGATIVE measures        \n","test_neg_data = {'train_test':['TEST'] * len(test_neg_file_names),\\\n","    'pos_neg': ['NEG'] * len(test_neg_file_names),\\\n","    'file_name': test_neg_file_names,\\\n","    'POSITIVE': positive, 'NEGATIVE': negative,\\\n","    'rating': rating}        \n","test_neg_data_frame = pd.DataFrame(test_neg_data)\n","\n","# merge the positive and negative testing data frames\n","test_data_frame = pd.concat([test_pos_data_frame, test_neg_data_frame],\\\n","    axis = 0, ignore_index = True)\n","\n","# determining thumbs up or down based on rating\n","test_data_frame['thumbsupdown'] = \\\n","    test_data_frame['rating'].map(map_to_thumbsupdown)\n","# compute simple measure of sentiment as POSITIVE - NEGATIVE\n","test_data_frame['simple'] = \\\n","    test_data_frame['POSITIVE'] - test_data_frame['NEGATIVE']    \n","# examine the data frame\n","print(pd.crosstab(test_data_frame['pos_neg'],\\\n","    test_data_frame['thumbsupdown']))\n","print(test_data_frame.head())\n","print(test_data_frame.tail())\n","print(test_data_frame.describe())\n","ratings_grouped = test_data_frame['simple'].\\\n","    groupby(test_data_frame['rating'])\n","print('\\nTest Data Simple Difference Means by Ratings:',\\\n","    ratings_grouped.mean())\n","thumbs_grouped = \\\n","    test_data_frame['simple'].groupby(test_data_frame['thumbsupdown'])\n","print('\\nTest Data Simple Difference Means by Thumbs UP/DOWN:',\\\n","    thumbs_grouped.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uYJYDG5_SZUi","colab_type":"code","colab":{}},"source":["# repeat methods for the Tom's movie reviews -----------------------------\n","# /reviews/test/tom/ testing set directory path\n","test_tom_file_names, test_tom_corpus, test_tom_aggregate_corpus = \\\n","    corpus_creator(input_directory_path = 'reviews/test/tom/',\\\n","        output_directory_path = 'reviews/test/tom_parsed/')\n","\n","# word counts for Tom's reviews\n","positive_words = []\n","negative_words = []\n","total_words = []\n","for file in test_tom_corpus:\n","    positive_words.append(count_positive(file)) \n","    negative_words.append(count_negative(file))       \n","    total_words.append(count_total(file))      \n","               \n","# POSITIVE and NEGATIVE measures/scores for Tom's reviews\n","positive, negative = compute_scores(test_tom_corpus)\n","rating = []\n","for file_name in test_tom_file_names:\n","    rating.append(get_rating(str(file_name)))\n","\n","# create data frame to check calculations of counts and scores        \n","test_tom_data = {'train_test':['TOM'] * len(test_tom_file_names),\\\n","    'pos_neg': ['POS', 'POS', 'NEG', 'POS', 'NEG', 'NEG', 'POS', 'NEG'],\\\n","    'file_name': test_tom_file_names,\\\n","    'movie': ['Marigolds',\\\n","    'Blade Runner',\\\n","    'Vinny',\\\n","    'Mars Attacks',\n","    'Fight Club',\\\n","    'Congeniality',\\\n","    'Find Me Guilty',\\\n","    'Moneyball'],\\\n","    'positive_words' : positive_words,\\\n","    'negative_words' : negative_words,\\\n","    'total_words' : total_words,\\\n","    'POSITIVE': positive, 'NEGATIVE': negative,\\\n","    'rating': rating}        \n","test_tom_data_frame = pd.DataFrame(test_tom_data)\n","\n","# determing thumbs up or down based upon rating\n","test_tom_data_frame['thumbsupdown'] = \\\n","    test_tom_data_frame['rating'].map(map_to_thumbsupdown)\n","# compute simple measure of sentiment as POSITIVE - NEGATIVE\n","test_tom_data_frame['simple'] = \\\n","    test_tom_data_frame['POSITIVE'] - test_tom_data_frame['NEGATIVE']    \n","\n","# examine the data frame\n","print(test_tom_data_frame)\n","print(test_tom_data_frame.describe())\n","ratings_grouped = test_tom_data_frame['simple'].\\\n","    groupby(test_tom_data_frame['rating'])\n","print('\\nTom Simple Difference Means by Ratings:',ratings_grouped.mean())\n","thumbs_grouped = \\\n","    test_tom_data_frame['simple'].groupby(test_tom_data_frame['thumbsupdown'])\n","print('\\nTom Simple Difference Means by Thumbs UP/DOWN:',\\\n","    thumbs_grouped.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tyt1rBiSZQh","colab_type":"code","colab":{}},"source":["# develop predictive models using the training data\n","# --------------------------------------\n","# Simple difference method\n","# --------------------------------------\n","# use the median of the simple difference between POSITIVE and NEGATIVE\n","simple_cut_point = train_data_frame['simple'].median()\n","\n","# algorithm for simple difference method based on training set median\n","def predict_simple(value):\n","    if (value > simple_cut_point):\n","        return('UP')\n","    else:\n","        return('DOWN')\n","\n","train_data_frame['pred_simple'] = \\\n","    train_data_frame['simple'].apply(lambda d: predict_simple(d))\n","print(train_data_frame.head())    \n","\n","print('\\n Simple Difference Training Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified:',\\\n","    100 * round(evaluate_classifier(train_data_frame['pred_simple'],\\\n","    train_data_frame['thumbsupdown'])[4], 3),'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5kzBv64-SZJ_","colab_type":"code","colab":{}},"source":["# evaluate simple difference method in the test set\n","# using algorithm developed with the training set\n","test_data_frame['pred_simple'] = \\\n","    test_data_frame['simple'].apply(lambda d: predict_simple(d))\n","print(test_data_frame.head())    \n","\n","print('\\n Simple Difference Test Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified:',\\\n","    100 * round(evaluate_classifier(test_data_frame['pred_simple'],\\\n","    test_data_frame['thumbsupdown'])[4], 3), '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vA914tRJThG5","colab_type":"code","colab":{}},"source":["# --------------------------------------\n","# Regression difference method\n","# --------------------------------------\n","# regression method for determining weights on POSITIVE AND NEGATIVE\n","# fit a regression model to the training data\n","\n","regression_model = str('rating ~ POSITIVE + NEGATIVE')\n","\n","# fit the model to the training set\n","train_regression_model_fit = smf.ols(regression_model,\\\n","    data = train_data_frame).fit()\n","# summary of model fit to the training set\n","print(train_regression_model_fit.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-CJY4cNTni5","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sAIyw_hYTnWJ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKFl0sYSTj60","colab_type":"code","colab":{}},"source":["# because we are using predicted rating we use the midpoint \n","# rating of 5 as the cut-point for making thumbs up or down predictions\n","regression_cut_point = 5\n","\n","# algorithm for simple difference method based on training set median\n","def predict_regression(value):\n","    if (value > regression_cut_point):\n","        return('UP')\n","    else:\n","        return('DOWN')\n","\n","# training set predictions from the model fit to the training set\n","train_data_frame['pred_regression_rating'] =\\\n","    train_regression_model_fit.fittedvalues\n","\n","# predict thumbs up or down based upon the predicted rating\n","train_data_frame['pred_regression'] = \\\n","    train_data_frame['pred_regression_rating'].\\\n","        apply(lambda d: predict_regression(d))\n","print(train_data_frame.head())    \n","\n","print('\\n Regression Difference Training Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified:',\\\n","    100 * round(evaluate_classifier(train_data_frame['pred_regression'],\\\n","    train_data_frame['thumbsupdown'])[4], 3),'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B3gLqVBvTrqz","colab_type":"code","colab":{}},"source":["# evaluate regression difference method in the test set\n","# using algorithm developed with the training set\n","# predict thumbs up or down based upon the predicted rating\n","\n","# test set predictions from the model fit to the training set\n","test_data_frame['pred_regression_rating'] =\\\n","    train_regression_model_fit.predict(test_data_frame)\n","\n","test_data_frame['pred_regression'] = \\\n","    test_data_frame['pred_regression_rating'].\\\n","        apply(lambda d: predict_regression(d))\n","print(test_data_frame.head())    \n","\n","print('\\n Regression Difference Test Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified:',\\\n","    100 * round(evaluate_classifier(test_data_frame['pred_regression'],\\\n","    test_data_frame['thumbsupdown'])[4], 3), '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mFi8tiSSTvDA","colab_type":"code","colab":{}},"source":["# --------------------------------------------\n","# Compute text measures for each corpus\n","# --------------------------------------------\n","# return to score the document collections with get_text_measures\n","# for each of the selected words from the sentiment lists\n","# these new variables will be given the names of the words\n","# to keep things simple.... there are 50 such variables/words\n","# identified from our analysis of the unsup corpus above\n","# \n","# start with the training document collection\n","working_corpus = train_pos_corpus + train_neg_corpus\n","add_corpus_data = get_text_measures(working_corpus)\n","add_corpus_data_frame = pd.DataFrame(add_corpus_data)\n","# merge the new text measures with the existing data frame\n","train_data_frame =\\\n","    pd.concat([train_data_frame,add_corpus_data_frame],axis=1) \n","# examine the expanded training data frame\n","print('\\n xtrain_data_frame (rows, cols):',train_data_frame.shape,'\\n')\n","print(train_data_frame.describe())\n","print(train_data_frame.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eww-PngsTx70","colab_type":"code","colab":{}},"source":["# start with the test document collection\n","working_corpus = test_pos_corpus + test_neg_corpus\n","add_corpus_data = get_text_measures(working_corpus)\n","add_corpus_data_frame = pd.DataFrame(add_corpus_data)\n","# merge the new text measures with the existing data frame\n","test_data_frame = pd.concat([test_data_frame,add_corpus_data_frame],axis=1) \n","# examine the expanded testing data frame\n","print('\\n xtest_data_frame (rows, cols):',test_data_frame.shape,'\\n')\n","print(test_data_frame.describe())\n","print(test_data_frame.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzW8doBaTzRo","colab_type":"code","colab":{}},"source":["# end with Tom's reviews as a document collection\n","working_corpus = test_tom_corpus\n","add_corpus_data = get_text_measures(working_corpus)\n","add_corpus_data_frame = pd.DataFrame(add_corpus_data)\n","# merge the new text measures with the existing data frame\n","tom_data_frame =\\\n","    pd.concat([test_tom_data_frame,add_corpus_data_frame],axis=1) \n","# examine the expanded testing data frame\n","print('\\n xtom_data_frame (rows, cols):',tom_data_frame.shape,'\\n')\n","print(tom_data_frame.describe())\n","print(tom_data_frame.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vhI1Fk_RT24T","colab_type":"code","colab":{}},"source":["# --------------------------------------------\n","# Word/item analysis method for training set\n","# --------------------------------------------\n","# item-rating correlations for all 50 words\n","\n","item_list = selected_positive_words + selected_negative_words\n","item_rating_corr = []\n","for item in item_list:\n","    item_rating_corr.\\\n","        append(train_data_frame['rating'].corr(train_data_frame[item]))\n","item_analysis_data_frame =\\\n","    pd.DataFrame({'item': item_list, 'item_rating_corr': item_rating_corr})    \n","# absolute value of item correlation with rating\n","item_analysis_data_frame['abs_item_rating_corr'] =\\\n","    item_analysis_data_frame['item_rating_corr'].apply(lambda d: abs(d))\n","    \n","# look at sort by absolute value \n","print(item_analysis_data_frame.sort_index(by = ['abs_item_rating_corr'],\\\n","    ascending = False))    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"98ma-s35T6ZC","colab_type":"code","colab":{}},"source":["# select subset of items with absolute correlations > 0.05\n","selected_item_analysis_data_frame =\\\n","    item_analysis_data_frame\\\n","        [item_analysis_data_frame['abs_item_rating_corr'] > 0.05]                    \n","\n","# identify the positive items for word/item analysis measure\n","selected_positive_item_df =\\\n","    selected_item_analysis_data_frame\\\n","        [selected_item_analysis_data_frame['item_rating_corr'] > 0]\n","possible_positive_items = selected_positive_item_df['item']\n","print('Possible positive items:',possible_positive_items,'\\n') \n","# note some surprises in the list of positive items \n","# select list consitent with initial list of positive words\n","selected_positive_items =\\\n","    list(set(possible_positive_items) & set(positive_word_list))\n","print('Selected positive items:',selected_positive_items,'\\n') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZN8pG3PT6Pr","colab_type":"code","colab":{}},"source":["# identify the negative items for word/item analysis measure\n","selected_negative_item_df =\\\n","    selected_item_analysis_data_frame\\\n","        [selected_item_analysis_data_frame['item_rating_corr'] < 0]\n","possible_negative_items = selected_negative_item_df['item']\n","print('Possible negative items:',possible_negative_items,'\\n') \n","# select list consitent with initial list of negative words\n","selected_negative_items =\\\n","    list(set(possible_negative_items) & set(negative_word_list))\n","print('Selected negative items:',selected_negative_items,'\\n') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aSsvoO2UT6HG","colab_type":"code","colab":{}},"source":["# the word \"funny\" remains a mystery... kept in negative list for now\n","\n","# selected positive and negative items entered into function\n","# for obtaining word/item analysis summative score in which\n","# postive items get +1 point and negative items get -1 point\n","# ... implemented in imported Python utility get_summative_scores\n","\n","# start with the training set... identify a cut-off\n","working_corpus = train_pos_corpus + train_neg_corpus\n","add_corpus_data = get_summative_scores(working_corpus)\n","add_corpus_data_frame = pd.DataFrame(add_corpus_data)\n","# merge the new text measures with the existing data frame\n","train_data_frame = pd.concat([train_data_frame,add_corpus_data_frame],axis=1) \n","# examine the expanded training data frame and summative_scores\n","print('\\n train_data_frame (rows, cols):',train_data_frame.shape,'\\n')\n","print(train_data_frame['summative_score'].describe())\n","print('\\nCorrelation of ratings and summative scores:'\\\n","    ,round(train_data_frame['rating'].\\\n","        corr(train_data_frame['summative_score']),3))\n","ratings_grouped = train_data_frame['summative_score'].\\\n","    groupby(train_data_frame['rating'])\n","print('\\nTraining Data Summative Score Means by Ratings:',\\\n","    ratings_grouped.mean())\n","thumbs_grouped = \\\n","    train_data_frame['summative_score'].\\\n","        groupby(train_data_frame['thumbsupdown'])\n","print('\\nTraining Data Summative Score Means by Thumbs UP/DOWN:',\\\n","    thumbs_grouped.mean())\n","# analyses suggest a simple positive/negative cut on summative scores\n","# algorithm for word/item method based on training set summative_scores\n","def predict_by_summative_score(value):\n","    if (value > 0):\n","        return('UP')\n","    else:\n","        return('DOWN')\n","\n","# evaluate word/item analysis method on training set\n","train_data_frame['pred_summative_score'] = \\\n","    train_data_frame['summative_score'].\\\n","    apply(lambda d: predict_by_summative_score(d))\n","\n","print('\\n Word/item Analysis Training Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified by Summative Scores:',\\\n","    100 * round(evaluate_classifier(train_data_frame['pred_summative_score'],\\\n","    train_data_frame['thumbsupdown'])[4], 3),'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xskaeeiPUEAi","colab_type":"code","colab":{}},"source":["# compute summative scores on test data frame\n","working_corpus = test_pos_corpus + test_neg_corpus\n","add_corpus_data = get_summative_scores(working_corpus)\n","add_corpus_data_frame = pd.DataFrame(add_corpus_data)\n","# merge the new text measures with the existing data frame\n","test_data_frame = pd.concat([test_data_frame,add_corpus_data_frame],axis=1) \n","\n","# evaluate word/item analysis method (summative score method) on test set\n","# using algorithm developed with the training set\n","test_data_frame['pred_summative_score'] = \\\n","    test_data_frame['summative_score'].\\\n","        apply(lambda d: predict_by_summative_score(d))\n","        \n","print('\\n Word/item Analysis Test Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified by Summative Scores:',\\\n","    100 * round(evaluate_classifier(test_data_frame['pred_summative_score'],\\\n","    test_data_frame['thumbsupdown'])[4], 3), '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTnWH8yeUGuU","colab_type":"code","colab":{}},"source":["# --------------------------------------\n","# Logistic regression method\n","# --------------------------------------\n","# translate thumbsupdown into a binary indicator variable y\n","# here we let thumbs up have the higher value of 1\n","thumbsupdown_to_binary = {'UP':1,'DOWN':0}\n","train_data_frame['y'] =\\\n","    train_data_frame['thumbsupdown'].map(thumbsupdown_to_binary)\n","\n","# model specification in R-like formula syntax\n","text_classification_model = 'y ~  beautiful +\\\n","    best + better + classic + enjoy + enough +\\\n","    entertaining + excellent +\\\n","    fans +  fun + good + great + interesting + like +\\\n","    love +  nice + perfect + pretty + right +\\\n","    top + well + won + wonderful + work + worth +\\\n","    bad + boring + creepy + dark + dead+\\\n","    death + evil + fear + funny + hard + kill +\\\n","    killed + lack + lost + mystery +\\\n","    plot + poor + problem + sad + scary +\\\n","    slow + terrible + waste + worst + wrong'\n","\n","# convert R-like formula into design matrix needed for statsmodels        \n","y,x = patsy.dmatrices(text_classification_model,\\\n","    train_data_frame, return_type = 'dataframe')    \n","\n","# define the logistic regression algorithm \n","my_logit_model = sm.Logit(y,x)\n","# fit the model to training set\n","my_logit_model_fit = my_logit_model.fit()\n","print(my_logit_model_fit.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQNk1hqcUGjE","colab_type":"code","colab":{}},"source":["# predicted probability of thumbs up for training set\n","train_data_frame['pred_logit_prob'] =\\\n","    my_logit_model_fit.predict(linear = False)\n","\n","# map from probability to thumbsupdown with simple 0.5 cut-off\n","def prob_to_updown(x):\n","    if(x > 0.5):\n","        return('UP')\n","    else:\n","        return('DOWN')\n","                    \n","train_data_frame['pred_logit'] =\\\n","    train_data_frame['pred_logit_prob'].apply(lambda d: prob_to_updown(d))\n","\n","print('\\n Logistic Regression Training Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified:',\\\n","    100 * round(evaluate_classifier(train_data_frame['pred_logit'],\\\n","    train_data_frame['thumbsupdown'])[4], 3),'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XkpSpennUPTc","colab_type":"code","colab":{}},"source":["# use the model developed on the training set to predict\n","# thumbs up or down reviews in the test set \n","# assume that y is not known... only x used from patsy\n","y,x = patsy.dmatrices(text_classification_model,\\\n","        test_data_frame, return_type = 'dataframe') \n","y = []  # ignore known thumbs up/down from test set... \n","# we want to predict thumbs up/down from the model fit to \n","# the training set... my_logit_model_fit       \n","test_data_frame['pred_logit_prob'] =\\\n","    my_logit_model_fit.predict(exog = x, linear = False)\n","test_data_frame['pred_logit'] =\\\n","    test_data_frame['pred_logit_prob'].apply(lambda d: prob_to_updown(d))        \n","\n","print('\\n Logistic Regression Test Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified:',\\\n","    100 * round(evaluate_classifier(test_data_frame['pred_logit'],\\\n","    test_data_frame['thumbsupdown'])[4], 3),'\\n')\n","\n","# --------------------------------------\n","# Support vector machines\n","# --------------------------------------\n","# fit the model to the training set\n","y,x = patsy.dmatrices(text_classification_model,\\\n","    train_data_frame, return_type = 'dataframe')    \n","  \n","my_svm = svm.SVC()  \n","my_svm_fit = my_svm.fit(x, np.ravel(y))\n","train_data_frame['pred_svm_binary'] = my_svm_fit.predict(x)\n","binary_to_thumbsupdown = {0: 'DOWN', 1: 'UP'}\n","train_data_frame['pred_svm'] =\\\n","    train_data_frame['pred_svm_binary'].map(binary_to_thumbsupdown)\n","\n","print('\\n Support Vector Machine Training Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified:',\\\n","    100 * round(evaluate_classifier(train_data_frame['pred_svm'],\\\n","    train_data_frame['thumbsupdown'])[4], 3),'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wyZ-KZMGUSok","colab_type":"code","colab":{}},"source":["# use the model developed on the training set to predict\n","# thumbs up or down reviews in the test set \n","# assume that y is not known... only x used from patsy\n","y,x = patsy.dmatrices(text_classification_model,\\\n","        test_data_frame, return_type = 'dataframe') \n","y = []  # ignore known thumbs up/down from test set... \n","test_data_frame['pred_svm_binary'] = my_svm_fit.predict(x)\n","test_data_frame['pred_svm'] =\\\n","    test_data_frame['pred_svm_binary'].map(binary_to_thumbsupdown)\n","\n","print('\\n Support Vector Machine Test Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified:',\\\n","    100 * round(evaluate_classifier(test_data_frame['pred_svm'],\\\n","    test_data_frame['thumbsupdown'])[4], 3),'\\n')\n","\n","# --------------------------------------\n","# Random forests\n","# --------------------------------------\n","# fit random forest model to the training data\n","y,x = patsy.dmatrices(text_classification_model,\\\n","    train_data_frame, return_type = 'dataframe')    \n","\n","# for reproducibility set random number seed with random_state\n","my_rf_model = RandomForestClassifier(n_estimators = 10, random_state = 9999)\n","my_rf_model_fit = my_rf_model.fit(x, np.ravel(y))\n","train_data_frame['pred_rf_binary'] = my_rf_model_fit.predict(x)\n","train_data_frame['pred_rf'] =\\\n","    train_data_frame['pred_rf_binary'].map(binary_to_thumbsupdown)\n","\n","print('\\n Random Forest Training Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified:',\\\n","    100 * round(evaluate_classifier(train_data_frame['pred_rf'],\\\n","    train_data_frame['thumbsupdown'])[4], 3),'\\n')\n","\n","# use the model developed on the training set to predict\n","# thumbs up or down reviews in the test set \n","# assume that y is not known... only x used from patsy\n","y,x = patsy.dmatrices(text_classification_model,\\\n","        test_data_frame, return_type = 'dataframe') \n","y = []  # ignore known thumbs up/down from test set... \n","test_data_frame['pred_rf_binary'] = my_rf_model_fit.predict(x)\n","test_data_frame['pred_rf'] =\\\n","    test_data_frame['pred_rf_binary'].map(binary_to_thumbsupdown)\n","\n","print('\\n Random Forest Test Set Performance\\n',\\\n","    'Percentage of Reviews Correctly Classified:',\\\n","    100 * round(evaluate_classifier(test_data_frame['pred_rf'],\\\n","    test_data_frame['thumbsupdown'])[4], 3),'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dnfdZBsiRnTE","colab_type":"code","colab":{}},"source":["# Suggestions for the student:\n","# Employ stemming prior to the creation of terms-by-document matrices.\n","# Try alternative positive and negative word sets for sentiment scoring.\n","# Try word sets that relate to a wider variety of emotional or opinion states.\n","# Better still, move beyond a bag-of-words approach to sentiment. Use\n","# the tools of natural language processing and define text features\n","# based upon combinations of words such as bigrams (pairs of words)\n","# and taking note of parts of speech.  Yet another approach would be\n","# to define ignore negative and positive word lists and work directly \n","# with identified text features that correlate with movie review ratings or\n","# do a good job of classifying reviews into positive and negative groups.\n","# Text features within text classification problems may be defined \n","# on term document frequency alone or on measures of term document\n","# frequency adjusted by term corpus frequency. Using alternative \n","# features and text measures as well as alternative classification methods,\n","# run a true benchmark within a loop, using hundreds or thousands of iterations.\n","# See if you can improve upon the performance of modeling methods by\n","# modifying the values of arguments to algorithms used here.\n","# Use various methods of classifier performance to evaluate classifiers.\n","# Try text classification for the movie reviews without using initial\n","# lists of positive an negative words. That is, identify text features\n","# for thumbs up/down text classification directly from the training set."],"execution_count":null,"outputs":[]}]}